# -*- coding: utf-8 -*-
"""Useful_code_snippets.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16-2cv1SUv4S7q3rhmuC8Vv3OpD9V0HIM

## Useful code snippets

# Approach Used:

Data source and processessing:\
Step1: Obtain a sitemap.xml file\
Step2: Read data from url's\
Step3: Process the data to remove unwanted information\

Indexing,Querying and Information Retrival.\
Step1: Create an Index on Pinecone with 1536 dims( default by OpenAI embedding model) \
Step2: Chunk the data from documents into required size and overlapping( 1028 tokens and 256 in my case)(keep it in multiples of 2)\
Step3: Create metadata(title: by summarizing the chunks) for optimised retrieval and upload them with embeddings to Pinecone \
Step4: Store the metadata and the respective text chunks in a .csv file\
Step5: Retrieve the top_k queries from Pinecone, Use the metadata to retrieve the text values for the respective metadata from the textchunks stored in .csv file\
Step6: Concat the chunks and send them with prompt, asking the model to generate response for the user's query using the given info.



Note: Why I am retrieving the texts separately from .csv ? \
      The latest updated of Pinecone has created some issues with its competency to be used with Langchain, Hence not supporting the default retrieval mechanism using RetrievalQA chain. Hence, have figured out a method to solve the issue./
"""

pip install pinecone-client pypdf unstructured langchain langchain-openai langchain-community

"""### Retrieving information from the website and processing it further"""

#retrieve urls from sitemap.xml file format

from bs4 import BeautifulSoup
import requests
import pandas as pd
import re



# Your XML-like document\
file_path="/content/sabic_url.txt"

with open(file_path,'r',encoding='utf-8') as document:
  dd=document.read()


# Parse the document with BeautifulSoup
soup = BeautifulSoup(dd, 'html.parser')

# Find all 'loc' tags and extract the URLs starting with 'http:'
urls = [loc.text for loc in soup.find_all('loc') if loc.text.startswith('http:')]

#extract content and title from the urls

def scrape_website(url):
    s=""
    try:
        # Send an HTTP GET request to the URL
        response = requests.get(url)

        # Check if the request was successful (status code 200)
        if response.status_code == 200:
            # Parse the HTML content with BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')

            # Extract text content from paragraphs (you can customize this)
            paragraphs = soup.find_all('p')
            title = soup.title.text

            for paragraph in paragraphs:
               s=s+str(paragraph.text)
            return [s,title]

        else:
            print(f"Failed to retrieve data. Status code: {response.status_code}")

    except Exception as e:
        print(f"An error occurred: {str(e)}")


text={'text':[],'title':[]}
for i in urls:
  d=scrape_website(i)
  text['text'].append(d[0])
  text['title'].append(d[1])

#storing titles and text as df for easier processing and access
data=pd.DataFrame(text)
data.head()

#preprocessing to remove unwanted information and spaces and combining it all as one document.

x='\r\n                                This website uses first and third party cookies (and equivalent technologies) to improve your experience on our site. Necessary cookies ensure that this site functions properly. We also use cookies to analyze how our site performs, understand your preferences and deliver tailored commercial content on this and other sites. For more information about which cookies we use, the information collected and SABIC’s purposes, please see our Cookie Notice. By clicking ‘Accept Cookies’ you agree to the use of such cookies. Alternatively you can manage which cookies are placed on your device by selecting \r\n                                Manage Cookies'
sentences_to_replace=[x,'\r\n\r\n        ','FDA approved.','Follow us to stay connected on our LinkedIn','Compare up to 4 gradesYou  already have 4 products for comparisonCopyright 2024 SABIC']#,'\n\n\nHome\n>\nProducts\n>\nMetals\n>','\n\n\nHome\n>\nProducts\n>\nChemicals\n>\r\n','\n\n\nHome\n>\nProducts\n>\nPolymers\n>\r\n','\r\n','\n\n\nHome\n>\nProducts\n>\nAgri-Nutrients\n>\nAgri-Nutrients\n>',

updated_text=""
for i in range(data.shape[0]):
  input_text=data['text'][i]
  for sentence in sentences_to_replace:
    input_text = re.sub(re.escape(sentence), ' ', input_text)
  updated_text+=input_text


#writing it into a txt format
file_path='/content/sabic_website_materials.txt'

with open(file_path,'w',encoding='utf-8') as document:
  de=document.write(updated_text)

#writing it into csv file.
pd.DataFrame({'context':[updated_text]}).to_csv('sabic_materials_data.csv',index=False)

"""### Using Pinecone Vector DB to store embeddings"""

#create an index

from pinecone import Pinecone, ServerlessSpec
import os

api_key=Your_api_key_pinecone

pc = Pinecone(api_key=api_key)

pc.create_index(name="genai-petro4", spec=ServerlessSpec(
        cloud="aws",
        region="us-west-2"),dimension=1536)

index=pc.Index('genai-petro4')

#preprocess text from the document

import re
import pdfplumber

def preprocess_text(text):
    # Replace consecutive spaces, newlines, and tabs
    text = re.sub(r'\s+', ' ', text)
    return text

def process_pdf(file_path):
    with pdfplumber.open(file_path) as pdf:
        text = ''
        for page in pdf.pages:
            text += page.extract_text()
    return preprocess_text(text)

pdf_file_path = './.pdf'
pdf_text = process_pdf(pdf_file_path)

#create embeddings with metadata and store them on pinecone

#Metadata: I have used OpenAI text summarization capability to create a summary of data chunks and use them as titles, to help with faster filtering and search optimization by the retirver.

#duplicate embeddings for metadata search
from openai import OpenAI
# client = OpenAI()

client=OpenAI(api_key="")


def create_embeddings(text):
    MODEL = 'text-embedding-ada-002'
    res = client.embeddings.create(input=[text], model=MODEL)
    return dict(dict(res)['data'][0])['embedding']

def upsert_to_pinecone(index_name, document_id, embedding,metadata):
    index = pc.Index(index_name)
    index.upsert(vectors=[(document_id,embedding,metadata)])

def generate_chat_completion(prompt):
    # Use OpenAI GPT for chat completion
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # Choose the appropriate OpenAI GPT engine
        max_tokens=15,
         messages=[
    {"role": "system", "content": "Your job is to understand the given inforamtion and create a title for it."},
    {"role": "user", "content": prompt}]# Adjust as needed
    )
    return response

# Load your PDF and process it
pdf_file_path = 'path/to/your.pdf'
pdf_text = process_pdf(pdf_file_path)

# Split the text into chunks (e.g., paragraphs)
chunks = pdf_text.split('\n\n')
ch=chunks[0][0:153172]


texts_orig={'text':[],'ids':[]}

# Create embeddings for each chunk and upsert to Pinecone
for i in range(0,149):
    if i==0:
        j1=0
        k1=1028
        c1=0
    else:
        j1=k1
        k1=k1+1028
        c1=j1-256
    t=ch[c1:j1]+ch[j1:k1]
    texts_orig['text'].append(t)
    texts_orig['ids'].append(f'chunk_{i}')
    chunk_embedding = create_embeddings(t)
    s=generate_chat_completion(t)
    metadata={"test":"chunk_"+str(i),"title":dict(s.choices[0])['message'].content}
    upsert_to_pinecone('genai-petro4', f'chunk_{i}', chunk_embedding,metadata)

##query the index and retrieve relevant texts and concat them

import pandas as pd
from openai import OpenAI
# client = OpenAI()

client=OpenAI(api_key="")


def create_embeddings(text):
    MODEL = 'text-embedding-ada-002'
    res = client.embeddings.create(input=[text], model=MODEL)
    return dict(dict(res)['data'][0])['embedding']

# orig_text=pd.DataFrame(texts_orig)

q="What are the products manufactured using petrochemicals?"

rets=index.query(
    vector=create_embeddings(q),
    top_k=3,
    include_metadata=True)


ids=[]
for i in range(len(rets['matches'])):
    ids.append(rets['matches'][i]['id'])

orig_text=pd.DataFrame(texts_orig)

ret_text=""
for i in ids:
    ret_text=ret_text + str(orig_text['text'][[orig_text.index[orig_text['ids'] == str(i)]][0][0]])

#use chat completion to provide informationa long with query in the prompt and get a response

def generate_chat_completion2(prompt):
    # Use OpenAI GPT for chat completion
    response = client.chat.completions.create(
        model="gpt-3.5-turbo-0125",  # Choose the appropriate OpenAI GPT engine
        max_tokens=512,
         messages=[
    {"role": "system", "content": "Your job is to take given information and provide the answer to the user's query"},
    {"role": "user", "content": prompt}]# Adjust as needed
    )
    return response

prompt=f""" The user's query is : {q} . The given information to answer the query is: {ret_text}"""


res=generate_chat_completion2(prompt)